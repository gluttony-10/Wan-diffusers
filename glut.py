import torch
import numpy as np
from diffusers import AutoencoderKLWan, WanImageToVideoPipeline, WanTransformer3DModel, GGUFQuantizationConfig, WanPipeline
from diffusers.utils import export_to_video, load_image
from transformers import CLIPVisionModel, UMT5EncoderModel
from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler
import argparse
import torch.nn.functional as F
from sageattention import sageattn
from diffusers.hooks import apply_group_offloading
from typing import Callable, List, Optional, Tuple, Union
from diffusers.models.attention_processor import Attention
import gradio as gr
import psutil
import datetime
import os
import random


parser = argparse.ArgumentParser() 
parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7891, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--mcp_server", action="store_true", help="æ˜¯å¦å¯ç”¨mcpæœåŠ¡")
parser.add_argument('--attn', type=str, default='sdpa', choices=['sdpa', 'sage'], help='åŠ é€Ÿç±»å‹')
parser.add_argument('--vram', type=str, default='low', choices=['low', 'high'], help='æ˜¾å­˜æ¨¡å¼')
parser.add_argument('--lora', type=str, default=None, help='loraæ¨¡å‹è·¯å¾„')
args = parser.parse_args()

print(" å¯åŠ¨ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾… bilibili@åå­—é±¼ https://space.bilibili.com/893892")
print(f'\033[32mPytorchç‰ˆæœ¬ï¼š{torch.__version__}\033[0m')
if torch.cuda.is_available():
    device = "cuda" 
    print(f'\033[32mæ˜¾å¡å‹å·ï¼š{torch.cuda.get_device_name()}\033[0m')
    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824
    print(f'\033[32mæ˜¾å­˜å¤§å°ï¼š{total_vram_in_gb:.2f}GB\033[0m')
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')
    if torch.cuda.get_device_capability()[0] >= 8:
        print(f'\033[32mæ”¯æŒBF16\033[0m')
        dtype = torch.bfloat16
    else:
        print(f'\033[32mä¸æ”¯æŒBF16ï¼Œä»…æ”¯æŒFP16\033[0m')
        dtype = torch.float16
else:
    print(f'\033[32mCUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥\033[0m')
    device = "cpu"


class WanAttnProcessor2_0:
    def __init__(self, attn_func):
        self.attn_func = attn_func
        if not hasattr(F, "scaled_dot_product_attention"):
            raise ImportError("WanAttnProcessor2_0 requires PyTorch 2.0. To use it, please upgrade PyTorch to 2.0.")

    def __call__(
        self,
        attn: Attention,
        hidden_states: torch.Tensor,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        rotary_emb: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        encoder_hidden_states_img = None
        if attn.add_k_proj is not None:
            encoder_hidden_states_img = encoder_hidden_states[:, :257]
            encoder_hidden_states = encoder_hidden_states[:, 257:]
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states

        query = attn.to_q(hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)

        query = query.unflatten(2, (attn.heads, -1)).transpose(1, 2)
        key = key.unflatten(2, (attn.heads, -1)).transpose(1, 2)
        value = value.unflatten(2, (attn.heads, -1)).transpose(1, 2)

        if rotary_emb is not None:

            def apply_rotary_emb(hidden_states: torch.Tensor, freqs: torch.Tensor):
                x_rotated = torch.view_as_complex(hidden_states.to(torch.float64).unflatten(3, (-1, 2)))
                x_out = torch.view_as_real(x_rotated * freqs).flatten(3, 4)
                return x_out.type_as(hidden_states)

            query = apply_rotary_emb(query, rotary_emb)
            key = apply_rotary_emb(key, rotary_emb)

        # I2V task
        hidden_states_img = None
        if encoder_hidden_states_img is not None:
            key_img = attn.add_k_proj(encoder_hidden_states_img)
            key_img = attn.norm_added_k(key_img)
            value_img = attn.add_v_proj(encoder_hidden_states_img)

            key_img = key_img.unflatten(2, (attn.heads, -1)).transpose(1, 2)
            value_img = value_img.unflatten(2, (attn.heads, -1)).transpose(1, 2)

            hidden_states_img = self.attn_func(
                query, key_img, value_img, attn_mask=None, dropout_p=0.0, is_causal=False
            )
            
            hidden_states_img = hidden_states_img.transpose(1, 2).flatten(2, 3)
            hidden_states_img = hidden_states_img.type_as(query)

        hidden_states = self.attn_func(
            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False
        )
        hidden_states = hidden_states.transpose(1, 2).flatten(2, 3)
        hidden_states = hidden_states.type_as(query)

        if hidden_states_img is not None:
            hidden_states = hidden_states + hidden_states_img

        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        return hidden_states
    

def set_sage_attn_wan(
        model: WanTransformer3DModel,
        attn_func,
):
    for idx, block in enumerate(model.blocks):
        processor = WanAttnProcessor2_0(attn_func)
        block.attn1.processor = processor


ATTNENTION = {
    "sage": sageattn,
    "sdpa": F.scaled_dot_product_attention,
}

MAX_SEED = np.iinfo(np.int32).max

os.makedirs("outputs", exist_ok=True)

onload_device = torch.device("cuda")
offload_device = torch.device("cpu")

# Available models: Wan-AI/Wan2.1-I2V-14B-480P-Diffusers, Wan-AI/Wan2.1-I2V-14B-720P-Diffusers
model_id = "models/Wan2.2-TI2V-5B-Diffuser"
vae = AutoencoderKLWan.from_pretrained(
    model_id, 
    subfolder="vae", 
    torch_dtype=torch.float32,
    low_cpu_mem_usage=False,
    ignore_mismatched_sizes=True
)
pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=dtype)
pipe.to(device)
if args.lora!="None":
    pipe.load_lora_weights(args.lora)
    print(f"åŠ è½½{args.lora}")
if args.vram=="high":
    pipe.vae.enable_tiling()
    pipe.enable_model_cpu_offload()
else:
    apply_group_offloading(pipe.text_encoder, onload_device=onload_device, offload_device=offload_device, offload_type="leaf_level")
    apply_group_offloading(pipe.image_encoder, onload_device=onload_device, offload_device=offload_device, offload_type="leaf_level")
    apply_group_offloading(pipe.transformer, onload_device=onload_device, offload_device=offload_device, offload_type="leaf_level")
    apply_group_offloading(pipe.vae, onload_device=onload_device, offload_device=offload_device, offload_type="leaf_level")

set_sage_attn_wan(pipe.transformer, ATTNENTION[args.attn])


def generate(
    image_input,
    prompt,
    negative_prompt,
    steps,
    nf,
    height, 
    width, 
    seed_param,
):
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    if seed_param<=-1:
        seed = random.randint(0, MAX_SEED)
    else:
        seed = seed_param

    if image_input is None:
        output = pipe(
            prompt=prompt, 
            negative_prompt=negative_prompt, 
            height=height, 
            width=width, 
            num_frames=nf*24+1, 
            guidance_scale=5.0,
            num_inference_steps=steps,
            generator=torch.Generator().manual_seed(seed)
        ).frames[0]
    else:
        image = load_image(image_input)
        output = pipe(
            image=image, 
            prompt=prompt, 
            negative_prompt=negative_prompt, 
            height=height, 
            width=width, 
            num_frames=nf*24+1, 
            guidance_scale=5.0,
            num_inference_steps=steps,
            generator=torch.Generator().manual_seed(seed)
        ).frames[0]
    export_to_video(output, f"outputs/{timestamp}.mp4", fps=24)
    return f"outputs/{timestamp}.mp4", seed

with gr.Blocks(theme=gr.themes.Base()) as demo:
    gr.Markdown("""
            <div>
                <h2 style="font-size: 30px;text-align: center;">Wan2.2</h2>
            </div>
            <div style="text-align: center;">
                åå­—é±¼
                <a href="https://space.bilibili.com/893892">ğŸŒbilibili</a> 
                |Wan2.2
                <a href="https://github.com/Wan-Video/Wan2.2">ğŸŒgithub</a> 
            </div>
            <div style="text-align: center; font-weight: bold; color: red;">
                âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
            </div>
            """)
    with gr.Row():
        with gr.Column():
            image_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="filepath", height=480)
            prompt = gr.Textbox(label="æç¤ºè¯ï¼ˆä¸è¶…è¿‡200å­—ï¼‰", value="Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.")
            negative_prompt = gr.Textbox(label="è´Ÿé¢æç¤ºè¯", value="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°")
            steps = gr.Slider(label="é‡‡æ ·æ­¥æ•°", minimum=1, maximum=100, step=1, value=50)
            nf = gr.Slider(label="ç”Ÿæˆæ—¶é•¿ï¼ˆç§’ï¼‰", minimum=3, maximum=10, step=1, value=5)
            height = gr.Slider(label="é«˜åº¦", minimum=256, maximum=2560, step=16, value=720)
            width = gr.Slider(label="å®½åº¦", minimum=256, maximum=2560, step=16, value=1280)
            seed_param = gr.Number(label="ç§å­ï¼Œè¯·è¾“å…¥æ­£æ•´æ•°ï¼Œ-1ä¸ºéšæœº", value=-1)
            generate_button = gr.Button("ğŸ¬ å¼€å§‹ç”Ÿæˆ", variant='primary')
        with gr.Column():
            video_output = gr.Video(label="Generated Video")
            seed_output = gr.Textbox(label="ç§å­")

    gr.on(
        triggers=[generate_button.click, prompt.submit, negative_prompt.submit],
        fn = generate,
        inputs = [
            image_input,
            prompt,
            negative_prompt,
            steps,
            nf,
            height, 
            width,
            seed_param,
        ],
        outputs = [video_output, seed_output]
    )

if __name__ == "__main__": 
    demo.launch(
        server_name=args.server_name, 
        server_port=args.server_port,
        share=args.share, 
        mcp_server=args.mcp_server,
        inbrowser=True,
    )